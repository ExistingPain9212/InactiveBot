name: Subreddit Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6:00 UTC

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 355  # 5 hours 55 minutes

    steps:
    - name: Checkout main repo
      uses: actions/checkout@v3

    - name: Checkout scraper-storage branch
      uses: actions/checkout@v3
      with:
        ref: scraper-storage
        path: storage

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: pip install praw

    - name: Run InactiveBot.py
      run: |
        mkdir -p storage
        cp storage/subreddits*.db . || true
        python InactiveBot.py
        cp subreddits*.db storage/ || true

    - name: Commit and push updated database
      if: success()
      run: |
        cd storage
        if [ -f "../ready-to-commit.txt" ]; then
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add subreddits*.db
          git commit -m "ðŸ”„ Updated subreddit database(s) after scrape"
          git push origin scraper-storage
        fi
